{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef54208c",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b0080",
   "metadata": {},
   "source": [
    "- achieves low bias low variance models by sequentially training trees\n",
    "- each tree in the sequence is fit to the residual from the fit of the previous tree\n",
    "    - residual = true response - predicted response\n",
    "\n",
    "- with each tree in the sequence, the residual values get smaller on average\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176b594",
   "metadata": {},
   "source": [
    "# Gradient Boosting Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67febe3f",
   "metadata": {},
   "source": [
    "start with:\n",
    "- all the training data\n",
    "- a number of trees (hyperparameter)\n",
    "- a lambda value, the same learning rate idea as in Adaboost (hyperparameter)\n",
    "\n",
    "train all trees sequentially:\n",
    "- the first tree on the actual training data (X) and responses (y)\n",
    "- the rest of the trees on X and the residuals from the previous tree\n",
    "- no tree/observation weights, just different response values for each tree\n",
    "\n",
    "the final gradient boosting fit is:\n",
    "- the sum of the scaled fit of all trees\n",
    "- the fit/boundary of each tree is scaled by $\\lambda$ the learning rate:\n",
    "    - usually between 0 and 1, can be higher\n",
    "    - higher $\\lambda$ (closer to 1), the fit/boundary of each tree is added with little attenuation -- less regularization\n",
    "    - lower $\\lambda$ (closer to 0), the fit/boundary of each tree is added with more attenuation -- more regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362561ab",
   "metadata": {},
   "source": [
    "base model should be high bias low variance, decision tree as the base model should be heavily regularized, possible with max_depth (hyperparameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20cb8aa",
   "metadata": {},
   "source": [
    "number of trees sohuld be picted carefully (hyperparameter)\n",
    "- too high: overfitting, too low: underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24699994",
   "metadata": {},
   "source": [
    "$\\lambda$ scales the fit/boundary of each tree (hyperparameter)\n",
    "- underfitting if too low, overfitting if too high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae9823",
   "metadata": {},
   "source": [
    "Gradient boosting also gives us the option to pick a subset without replacement of the training data for each tree in the sequence\n",
    "\n",
    "for each tree:\n",
    "- if an observation is seen before, its remaining residual is used as athe response\n",
    "- if an observation has not been seen before, the actual response is used\n",
    "\n",
    "purposes:\n",
    "- faster runtime (sklearn gradient boosting model is very slow)\n",
    "- further regularization of the model -- the trees will be kept from collaboratively fitting to noise, outliers, etc\n",
    "\n",
    "in sklearn this is controlled by the subsample hyperparameter:\n",
    "- a value between 0 and 1\n",
    "- underfitting if too low, trees wouldn't see enough data to train on\n",
    "- overfitting (possibly) if too high since trees will see the same noise, outliers, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93c70a",
   "metadata": {},
   "source": [
    "## Gradient boosting -- Cost Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90274",
   "metadata": {},
   "source": [
    "the performance improves if each tree sees the gradient of the cost of the previous tree with respect to the predicted response\n",
    "\n",
    "MSE = $\\frac{1}{N} \\sum_{i=1}^{N} \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2$\n",
    "\n",
    "$\\frac{\\partial \\text{MSE}}{\\partial \\hat{y}^{(i)}} = -\\frac{2}{N} \\sum_{i=1}^{N} \\left( y^{(i)} - \\hat{y}^{(i)} \\right)$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
