{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2f08a0",
   "metadata": {},
   "source": [
    "## XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7b52b",
   "metadata": {},
   "source": [
    "- theory behind all 3 models are based on Gradient Boosting with some extensions\n",
    "- many useful programming functionalities were added that:\n",
    "    - reduce model variance further (better cross-validation and test performance)\n",
    "    - make the runtime faster (more efficient implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a273ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdc48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainf = pd.read_csv(\"/Users/vaibhavrangan/Downloads/Stat_303-3/Datasets/Car_features_train.csv\")\n",
    "trainp = pd.read_csv(\"/Users/vaibhavrangan/Downloads/Stat_303-3/Datasets/Car_prices_train.csv\")\n",
    "train = pd.merge(trainf, trainp)\n",
    "\n",
    "testf = pd.read_csv(\"/Users/vaibhavrangan/Downloads/Stat_303-3/Datasets/Car_prices_test.csv\")\n",
    "testp = pd.read_csv(\"/Users/vaibhavrangan/Downloads/Stat_303-3/Datasets/Car_features_test.csv\")\n",
    "test = pd.merge(testf, testp)\n",
    "\n",
    "predictors = ['mpg', 'engineSize', 'year', 'mileage']\n",
    "target = 'price'\n",
    "X_train = train[predictors]\n",
    "y_train = train[target]\n",
    "X_test = test[predictors]\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773bda4",
   "metadata": {},
   "source": [
    "- XGBoost is faster than Gradient Boosting because:\n",
    "    - parallel processing of the NODES OF EACH TREE\n",
    "    - \"histogram features\": while finding the decision rule at each node:\n",
    "        - take a predictor, create a histogram of its values\n",
    "        - find a threshold using these categorized values\n",
    "        - this method returns very comparable performance but much faster\n",
    "\n",
    "- XGBoost performs better than Gradient Boosting because of the extra hyperparameters:\n",
    "    - will be introduced in the model inputs\n",
    "    - strongly force the model to avoid overfitting\n",
    "    - make the model ignore outliers as much as possible\n",
    "\n",
    "- Does XGBoost miss anything? Yes, Huber loss:\n",
    "    - Gradient Boosting implements Huber loss very well\n",
    "    - XGBoost has \"pseudo Huber loss,\" not implemented well\n",
    "    - just use MAE in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65783370",
   "metadata": {},
   "source": [
    "# Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f53292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    random_state = 12,\n",
    "    max_depth = 6, # identical to AdaBoost and Gradient Boosting\n",
    "    learning_rate = 0.1, # same as AdaBoost and Gradient Boosting\n",
    "    subsample = 0.8, # same as AdaBoost and Gradient Boosting\n",
    "\n",
    "    # the extra hyperparameters\n",
    "    reg_lambda = 1, # the factor multiplied with the Ridge penalty added to the cost function of each tree\n",
    "    # the ridge penalty consists of \"leaf weights\" -- measuring the important of a leaf in the tree\n",
    "        # proportional to the MSE/Gini it reduces, inversely proportional to the complexity it adds\n",
    "    \n",
    "    gamma = 0.1,# \"tree pruning\" hyperparameter, cancels/prunes teh relatively unnecessary leaves by checking their leaf weights and eliminating the leaves with weights below gamma\n",
    "\n",
    "    colsample_bytree = 0.5, # a feature/predictor subset that each tree sees, tries to break any excess correlation between trees\n",
    "\n",
    ")\n",
    "# a group of three hyperparams:\n",
    "# colsample_bytree: the predictor subset size seen by each tree\n",
    "# colsample_byLevel: the predictor subset size seen by each level of the tree\n",
    "# colsample_bynode: the predictor subset size seen by each node of the tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
